%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}
\RequirePackage[usenames, dvipsnames]{xcolor}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}%, algorithm2e}
\usepackage[noend]{algorithmic}

\usepackage{tikz, xcolor}
\usetikzlibrary{arrows,shapes, positioning, matrix, patterns}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\journal{Journal XXX}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{When to use Hamming distance and k-mismatches algorithms using fast Fourier transform?}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{Tatiana Rocher}

\address{University of Bristol, UK}

\begin{abstract}
%% Text of abstract
?????
\end{abstract}

\begin{keyword}
Hamming distance \sep fast Fourier transform
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
\linenumbers

%% main text
\section*{Introduction}
\label{S:1}

\textcolor{red}{not good}

The last decades saw a lot of algorithm improvement coming.
The expected time consumption is most of the time based on the algorithm's worst complexity.

We wanted to know if the k-mismatching algorithm of Gawrychowski and Uznanski \cite{Kmism3} could be used.
We decided to compare it to some of it predessessors.

We gathered 6 algorithms, responding to 4 problems.
Every algorithm responding to a problem were compared to the others and to the naive algorithm.

The section \ref{Algo} presents the selection of algorithms.
Then we explain in the section \ref{Method} our implementation procedure and the datasets we used.
Finally, the section \ref{Res} present our tests and results.



\section{Algorithms}
\label{Algo}

\textcolor{red}{BAD!!}

The algorithms use between $1$ and $3$ of the following methods: the FFT, Kangaroo or RLE methods.
All algorithms except the k-mismatching $1$ use the FFT method,
all k-mismatching use the Kangaroo method
and the k-mismatching $3$ uses the RLE method.
Those relations are summarized in the figure \ref{AllAlgorithms}.

\textcolor{red}{$\Sigma$ is the text and pattern alphabet}

\textcolor{red}{division en blocs de taille $2m$ partout}


\begin{figure}[h]\centering
\begin{tikzpicture}[scale=1.5, every node/.style={scale=1.5}]
\input{./figure.tex}
\test
\end{tikzpicture}
\caption{
The methods are in blue, the algorithms in orange.
A link indicates that the algorithm uses the linked method (or algorithm).
}\label{AllAlgorithms}
\end{figure}

We will present the three methods, and the algorithms.


\subsection{FFT method}

The fast Fourier transform (FFT) can be used for vectors multiplication
as computing the addition of two FFT give the same result faster.
Stringology uses the FFT for alignement calculation
using bit vectors to represent the presence of absence of a given letter for every position.

The FFT method on a letter $\alpha$ runs as follow:
we compute two bit vectors matching the text and pattern
such that a $1$-bit appears when the letter $\alpha$ does,
we compute the FFT of both vectors
we add the text's FFT and the inverse pattern's FFT
and compute the inverse FFT of the result.
See Algorithm \ref{MethodFFT}.

The result shows the number of matching $\alpha$ at every alignment.
In pattern matching algorithm, this method can be used with every different letter
to compute the Hamming distance between two texts.

This method runs in $O(n \log n)$ times for one letter.



\begin{algorithm}
\caption{$HD\_FFT(T, P, \alpha)$: }
\label{MethodFFT}
\begin{algorithmic}[1]
\REQUIRE T of size $n$, P of size $m$
\STATE init $B_t[n]$, $B_p[m]$ \COMMENT {\texttt{bit vectors}}
\STATE init $Res[n]$
\FOR {$i$ from $0$ to $n$} 
	\STATE {$B_t[i] == \alpha? B_t[i] = 1 : B_t[i] = 0$} 
\COMMENT {\texttt{Match every }$\alpha$ \texttt{letter of }$T$}
	% \STATE $B_t[i] = 1$ 
	% \ELSE
	% \STATE $B_t[i] = 0$
	% \ENDIF
\ENDFOR
\FOR {$i$ from $0$ to $m$} 
	\STATE {$B_p[i] == \alpha? B_p[i] = 1 : B_p[i] = 0$}  \COMMENT {\texttt{Match every $\alpha$ letter of $P$}}
	% \IF {$B_p[i] == \alpha$}
	% \STATE $B_p[i] = 1$
	% \ELSE
	% \STATE $B_p[i] = 0$
% \ENDIF
\ENDFOR
\STATE $FFT_t = FFT(B_t)$ \COMMENT {\texttt{Compute the FFT of $B_t$}}
\STATE $FFT_p = FFT(B_p)$
\STATE $FFT_r = FFT_t + Reverse(FFT_p)$
\STATE $Res = InvFFT(FFT_r)$ \COMMENT {\texttt{Compute the inverse FFT}}
% % \ENDFOR
\RETURN $Res$

\end{algorithmic}
\end{algorithm}



\subsection{Exact Hamming distance}
\label{eHD}

Abrahamson \cite{ExactHD} designed the first pattern matching algorithm using FFTs.
The FFT method is used for every unique letter of the pattern, while adding the temporary results.
We have the number of matching letters for each alignment.
We can calculate the Hamming distance between the text and the pattern 
by reducing each position's result from the pattern size.
This algorithm runs in $O(n\sigma m \log n)$ times
(in $O(n\sigma m \log m)$ if we divide the text in blocs of size $2m$).

A way to optimize this algorithm is to put a threshold $\ell = \sqrt{m \log n}$
to sort the pattern letters as frequent or infrequent,
where a frequent letter appears more than $\ell$ times in the pattern.
The FFT method is used on the frequent letters.
We initialize a vector $A$, and do a single read of the text:
if we read an infrequent letter $\alpha$ at position $i$, we increment $A[i-k]$,
where $k$ is every occurrence of $\alpha$ in the pattern.
% See algorithm \ref{ExHD}.
This algorithm runs in $O(n \sqrt{m} \log m)$ times.


% \textcolor{red}{Q: Do we write every algorithm this way: pseudo-code and explanation?}


% \begin{algorithm}
% \label{ExHD}
% \caption{$ExHD(T, P)$: Compute the exact Hamming distance between $T$ and $P$}
% \begin{algorithmic}
% \REQUIRE {$T$ of size $n$, $P$ of size $m$, $\Sigma$}
% \STATE init $Res[n-m+1]$, $cpt[\Sigma]$
% \STATE $\ell = sqrt(m \log n)$
% \STATE $CountLetters(P, \ell)$ \COMMENT {$O(m)$}
% \FOR {every $\alpha$ in $\Sigma$}
% 	\IF {$isFrequent(\alpha)$}
% 		\STATE $Tmp = HD\_FFT(T, P, \alpha)$
% 		\FOR {$i$ in $[0, n-m+1]$}
% 			\STATE $Res[i] += Tmp[i]$
% 		\ENDFOR
% 		\ENDIF
% \ENDFOR
% \FOR {$i$ in $[0, n]$}
% 	\IF {$isInfrequent(T[i])$}
% 		\FOR {all $P[k] == T[i]$}
% 			\STATE $Res[i - k]++$ \COMMENT {positions $k$ are precomputed during $CountLetters$}
% 		\ENDFOR
% 	\ENDIF
% \ENDFOR
% \FOR {$i$ in $[0 ,n]$}
% 	\STATE $Res[i] = m - Res[i]$
% \ENDFOR
% \RETURN $Res$

% \end{algorithmic}
% \end{algorithm}




\subsection{Approximate Hamming distance}

Kopelowitz and Porat \cite{ApproxHD} proposed an algorithm to approximate the Hamming distance
using the exact Hamming distance algorithm (see section \ref{eHD}).
For a $1 \pm \epsilon$ approximation,
the algorithm runs $c+\log m$ iterations of the following loop:
\begin{itemize}
\setlength\itemsep{0.1em}
\item find a random mapping on $2 / \epsilon$ letters
\item compute the Hamming distance of $T$ and $P$, both mapped on this alphabet
\item keep  the bigger value for every position
\end{itemize}

This algorithms runs in $O(n / \epsilon \log n \log m)$ times.
% See algorithm \ref{AppHD}.


% \begin{algorithm}
% \label{AppHD}
% \caption{$AppHD(T, P, \epsilon)$: Compute an $(1 \pm \epsilon)$ approximate Hamming distance between $T$ and $P$}
% \begin{algorithmic}

% \REQUIRE {$T$ of size $n$, $P$ of size $m$, $\Sigma$, $\epsilon$}
% \STATE init Res
% \FOR {$i$ in $0, c * \log m]$}
% 	\STATE $Map = RandomMapping(\Sigma, 2 / \epsilon)$
% 	\STATE $Tmp = ExHD(Map(T), Map(P))$
% 	\FOR {$j$ in $[0, n-m+1]$}
% 		\IF {$Tmp[j]>Res[j]$}
% 			\STATE $Res[j]=Tmp[j]$
% 		\ENDIF
% 	\ENDFOR
% \ENDFOR
% \RETURN Res

% \end{algorithmic}
% \end{algorithm}




\subsection{Exact wildcard matching}

In $2006$, Clifford and Clifford \cite{WC}, proposed an algorithm to solve
exact matching with wildcards.
Here, every letter is replaced by a number, with $0$ beeing a wildcard.

The idea is to compute the sum of squared differences between the text and pattern
at every alignment.
The wildcard are taken into account by multiplying the difference by the pattern and text letters.
In other words, the algorithm computes $T[i] \times P[i+j] \times (T[i] - P[i+j])^2$
for every position $j$ of every alignment $i$.

The algorithm uses the pattern and text's FFTs, and theirs squared and cubed FFTs:
$FFT^2_P$ is the pattern's FFT where all its coefficient are the cubed coefficient of $FFT_P$.
The algorithm computes: 
\begin{itemize}
\setlength\itemsep{0.1em}
\item $a = FFT_T+ FFT^3_P$
\item $b = FFT^2_T+ FFT^2_P$
\item $c = FFT^3_T+ FFT_P$.
\item $Res = InvFFT(a) - 2 \times InvFFT(b) + InvFFT(c)$
\end{itemize}

Every position $i$ where $Res[i] = 0$ is an exact matching.
This algorithms runs in $O(n \log m)$ times.
% See algorithm \ref{WC}.



% \begin{algorithm}
% \label{WC}
% \caption{$WC(T, P)$: Find where $P$ matches in $P$}
% \begin{algorithmic}

% \REQUIRE {$T$ of size $n$, $P$ of size $m$}
% \STATE Compute $FFT_T$ and $FFT_P$
% \STATE Compute $FFT^2_T$, $FFT^3_T$, $FFT^2_P$ and $FFT^3_P$ \COMMENT {in $O(n)$ and $O(m)$}
% \STATE $a = FFT_T+ FFT^3_P$
% \STATE $b = FFT^2_T+ FFT^2_P$
% \STATE $c = FFT^3_T+ FFT_P$
% \STATE $Res = InvFFT(a) - 2*InvFFT(b) + InvFFT(c)$
% \RETURN $Res$

% \end{algorithmic}
% \end{algorithm}


\subsection{Kangaroo method}

The Kangaroo method \textcolor{red}{who do we cite ??}
computes the Hamming distance between two words, up to $k$.
It uses the longest common prefix (LCP) between two words.



Here we designed the Kangaroo method to finds the LCP between an alignment of $T$ and $P$
(starting at $T[i]$ and $P[0]$).
Let says this LCP is of length $p$, we have $T[i+p] \neq P[p]$.
Then the method looks for the following LCP starting at position $T[i+p+1]$ and $P[p+1]$.
See algorithm \ref{AlgoKang}.
The result is the Hamming distance if we look for the LCP until we reach the end of the pattern,
but it is $k$ if we don't.

It needs a structure which returns the LCP between any suffix of the text and any suffix of the pattern
in $O(1)$ times.
This method runs in $O(k)$ times.

% \textcolor{red}{I put the kangaroo and RLE methods here so they are close to the algorithms using them.}



\begin{algorithm}
\caption{$Kangaroo(T, P, i, k)$}
\label{AlgoKang}
\begin{algorithmic}

\REQUIRE {$T$ of size $n$, $P$ of size $m$, position $i$ of the alignment, number of errors allowed $k$}
\STATE init struct\_LCP
\STATE $nb\_err = 0$, $pos_P = 0$, $pos_T = i$
\WHILE {$nb\_err <=k$ OR $pos_P < m$}
		\STATE $p = FindLCP(pos_T, pos_P)$
		\COMMENT {returns the size of the LCP}
		\STATE $pos_T += p + 1$
		\STATE $pos_P += p +1$
		\STATE $nb\_err++$
		\ENDWHILE
\RETURN $nb\_err$

\end{algorithmic}
\end{algorithm}







\subsection{K-mismatching 1}

This first k-mismatching algorithm, proposed by Landau and Vishkin\cite{Kangaroo}
uses the Kangaroo method for every alignment:
for every position in $T$, it looks for the LCP up to $k$ times or until we reach the end of the pattern.

This algorithm runs in $O(kn)$ times.
% See algorithm \ref{K1}.



% \begin{algorithm}
% \label{K1}
% \caption{$K\_mism\_1(T, P, k)$}
% \begin{algorithmic}

% \REQUIRE {$T$ of size $n$, $P$ of size $m$, $k$}
% \STATE init $Res[n-m+1]$
% \STATE init $Struct\_LCP$
% \FOR {$i$ in $[0, n-m+1]$}
% 	\STATE $pos_T = i$
% 	\STATE $pos_P = 0$
% 	\STATE $e=0$
% 	\WHILE {$e <=k$ OR $pos_P < m$}
% 		\STATE $p = FindLCP(pos_T, pos_P)$
% 		\STATE $pos_T += p + 1$
% 		\STATE $pos_P += p +1$
% 		\STATE $e++$
% 		\ENDWHILE
% 	\STATE $Res[i] = e$
% \ENDFOR
% \RETURN $Res$

% \end{algorithmic}
% \end{algorithm}


\subsection{K-mismatching 2}
Amir, Lewenstein and Porat\cite{Kmism2} proposed a k-mismatching algorithm
using both the FFT and Kangaroo methods.
This algorithm starts by sorting the pattern's letters as frequent or infrequent,
where a frequent letter appears more than $\sqrt{k}$ times.

If there is less than $2\sqrt{k}$ frequent letters: 
we use the FFT method with all the frequent letters and count the matching on infrequent letters directly.

If there is more than $2\sqrt{k}$ frequent letters:
we select $\sqrt{k}$ occurrences of $2\sqrt{k}$ frequent letters,
theirs positions gathered in a list $J$.
We initialise a vector $d$.
Then, for each position $i$ in $T$ and every value $j$ in $J$ such than $T[i] = P[j]$,
we increase $d[i-j]$.
Then, for each position $i$ such that $d[i]>k$, we use the Kangaroo method.

This algorithm runs in $O(n\sqrt{k} \log k)$ times.


% \begin{algorithm}
% \label{K2}
% \caption{$K\_mism\_2(T, P, k)$}
% \begin{algorithmic}

% \REQUIRE {$T$ of size $n$, $P$ of size $m$, $k$}
% \STATE init $Res[n-m+1]$
% \STATE $SortLetters(P, 2\sqrt{k})$ \COMMENT {$O(m)$}
% \IF {$nbFreqLetters < 2*\sqrt{k}$}
% 	\FOR {every $\alpha$ in $frequentLetters$}
% 		\STATE $Tmp = HD\_FFT(T, P, \alpha)$
% 		\FOR {$i$ in $[0, n-m+1]$}
% 			\STATE $Res[i] += Tmp[i]$
% 		\ENDFOR
% 	\ENDFOR
% 	\FOR {$i$ in $[0, n]$}
% 		\IF {$isInfrequent(T[i])$}
% 			\FOR {all $P[k] == T[i]$}
% 				\STATE $Res[i - k]++$ \COMMENT {positions $k$ are precomputed during $CountLetters$}
% 			\ENDFOR
% 		\ENDIF
% 	\ENDFOR
% \ELSE
% 	\STATE init Struct\_LCP
% 	\STATE init $J$
% 	\FOR {$j$ in $[0, m]$}
% 		\IF {}
% 			\STATE $J.add[j]$
% 		\ENDIF
% 	\ENDFOR
% 	\FOR {$i$ in $[0, n]$}
% 		\FOR {}

% 		\ENDFOR
% 	\ENDFOR

% \ENDIF

% \RETURN $Res$

% \end{algorithmic}
% \end{algorithm}





\subsection{RLE method}

The RLE method computes the Hamming distance between two words
by processing caracter repetitions (run) instead of single letters.  
Let $RLE_T$ and $RLE_P$ be the run length encoding of $T$ and $P$.
This method uses every pair of text-pattern runs of the same letter
to update the second derivative of the Hamming distance,
see Algorithm \ref{RLEmet}.

The methods runs in $O(r_T \times r_P)$ times,
where $r_T$ is the number of runs of T and $r_T$ is the number of runs of T.


\begin{algorithm}
\label{RLEmet}
\caption{$RLE(RLE_T, RLE_P)$}
\begin{algorithmic}

\REQUIRE {$T$ of size $n$, $P$ of size $m$}
\STATE init $A[n-m+1]$
\FOR {every run $r$ in $RLE_T$}
	\FOR {every run $s$ in $RLE_P$}
		\IF {$r.letter == s.letter$}
			\STATE $A[r.begin - s.end] ++$
			\STATE $A[r.end-s.end+1] --$
			\STATE $A[r.begin - s.begin+1]--$
			\STATE $A[r.end - s.begin +2] ++$
		\ENDIF
	\ENDFOR
\ENDFOR
\STATE $Res[0] = HD(T, P, 0)$
\STATE $Res[1] = HD(T, P, 1)$
\FOR {$i$ in $[2, n-m+1]$}
	\STATE $Res[i] = -A[i] + 2 * Res[i-1] - Res[i-2]$
\ENDFOR
\RETURN $Res$

\end{algorithmic}
\end{algorithm}






\subsection{K-mismatching 3}
Gawrychowski and Uznanski\cite{Kmism3} proposed the last k-mismatching algorithm of our selection.
It uses the FFT, Kangaroo and RLE methods. 

First, run a $(1 \pm \epsilon)$ approximate algorithm on the pattern against itself
to find a $O(k)$-period of the pattern.

If there is no $4k$-period smaller than $k$,
we run the $(1 \pm \epsilon)$ approximate algorithm on the pattern against the text,
we save the positions where the algorithm reports at most $k$ mismatches,
and we use the Kangaroo method on this positions. 

If there is: we have $l$, the minimal $8k$-period of $P$.
We compute $P^*$ and $T^*$, the run length encoding of the reorganisation of $P$ and $T$ using $l$
(see \cite{Kmism3} for further explanations).
We sort the characters of $P^*$ as frequent or infrequent characters,
where a frequent character appears in more than $t$ runs.
% ($t$ is undetermined in the paper).
We use a FFT for every frequent character,
and use the RLE method on infrequent characters.

This algorithm runs in $O(m \log^2 m \log \sigma +k \sqrt{m \log m} * n/m)$ times.










\section{Method}
\label{Method}

\subsection*{Implementation}

All the algorithms were implemented in C++.

We compared $8$ methods from $3$ libraries to find how to do the fastest FFT multiplication:
AlgLib (free edition) \cite{alglib}, FFTW3 \cite{FFTW05} and Flint \cite{Hart2010}.
The Flint library computes a polynomial multiplication while
the AlgLib and FFTW3 libraries use the FFT method. %for two vectors,
%adds the results and computes the inverse FFT of the result.

We tested the execution time of every method on vectors of $1$ million random integers.
The table \ref{tempsFFT} presents the average time of 10 executions.


\begin{table}[h]
\label{tempsFFT}
\begin{tabular}{|c|c|c|}
\hline
Library 				& Function used 							& Execution time (s) \\ \hline
AlgLib 					& fftr1d 									& $0.51$ \\ \hline
\multirow{2}{*}{FFTW3} 	& FFTW\_plan\_dft\_r2c\_1d (without saved plan) & $0.14$ \\
						& FFTW\_plan\_dft\_r2c\_1d (with saved plan) 	& $0.1$ \\ \hline
\multirow{5}{*}{Flint} 	& fmpz\_poly\_mul 							& $0.24$ \\
						& fmpq\_poly\_mul 							& $0.24$ \\
						& fq\_poly\_mul\_classical 					& $> 100$ \\
						& fq\_poly\_mul\_reorder 						& $1.17$ \\
						& fq\_poly\_mul\_KS 							& $1.50$ \\ \hline
\end{tabular}
\caption{Execution time of polynomial multiplication of vectors of $1$ millions integers,
using several methods from the Flint, AlgLib and FFTW3 libraries.
}
\end{table}

Dues to is fast execution times, we used the FFTW3 library.
The FFT computation of FFTW3 uses a \textit{plan} which find the best method to compute the FFT of a given size.
The number of inspected methods vary with the flag used.
There are four flags, which sorted by number of tested methods in increasing order are:
estimate, mesure, patient and exhaustive.
A plan can be precomputed, saved and reused later.

Its documentation recommends to use FFT for vector which size is a power of $2$.
We computed and saved all FFT plans from $1$ number to $16$ millions numbers for our futur uses.
Except if specified, all tests use plan with the patient flag.


\subsection*{Dataset}

Most of the tests were made on real data:
\begin{itemize}
\setlength\itemsep{-0.4em}
\item a DNA file composed of the full genome e.coli (4.6 Mb). A concatenation of this text with a copy of itself was made to increase its leangth.
\item an English text composed of five books from the Sherlock Holmes serie from Sir Arthur Conan Doyles (8 Mb)
\end{itemize}
% For the tests using this files, the text was a prefixe of the wanted size, 
% the pattern was extracted from the file at a random position. \textcolor{red}{XXbaaad!!XX}

% We made artificial texts to force a characteristic.
% Each one will be described in the result section.




\section{Results}
\label{Res}


\textcolor{red}{what do we tell here ?}

Each algorithm was compared with at least a naive algorithm.
Note that comparison beween two algorithm won't include the writing time.


- Most of the algorithms use a threshold to use a method or another. We will test all of this threshold.

- We will test te execution time on real DNA and English text for every algorithms.


Tests were made on a computer Intel® Core™ i7-6700 CPU @ $3.40$GHz.


\subsection{Exact matching using Wildcard}
\label{WCTests}

We want to know how to use the FFT method:
should we divide the text in substrings?
How much time do we save by using the saved plan?

This algorithm was tested on randomly generated files of numbers from $0$ to $10$.
The text was composed of $8$ millions numbers and the pattern of $100$ thousand numbers.
% The naive algorithm runs in more than $10$ minutes.

The algorithm ran several times with several options:
when the text is computed as one bloc or divided in blocs of size $2m$
and with or without charging the saved plan (with the patient flag).
Note that is we don't use a saved plan, the algorithm creates a plan with the estimate flag.
The table \ref{TableWC} shows the execution times with every option.

\begin{table}[h]
\begin{tabular}{|c|c|c|}
\hline
			& without saved plan & with saved plan \\ \hline
full text	& $7.84$ s & $5.95$ s \\ \hline
\begin{tabular}{@{}c@{}}text divided in blocs \\ of size $2m$\end{tabular} &  $5.83$ s & $5.59$ s \\ \hline
\end{tabular}
\caption{Excecution times of the exact matching using wildcards algorithm.
Text of $8$ millions numbers, pattern of $100$ thousand numbers}
\label{TableWC}
\end{table}


Note that the plans with the patient flag were computed in
$90$ minutes for a FFT of $8$ millions letters
and in $33$ seconds for a FFT for $200$ thousands letters.

This algorithm runs faster if the text is divided in smaller blocs
and obviously if we use a plan already computed.

In the other hand, if the patient plan wasn't precomputed,
it is better to not force the algorithm to use a plan with a patient flag.

Moreover, it is often faster to compute a small plan with the exhaustive flag
than a bigger one with the patient flag, making the execution even faster.
In the example above, we could pre-compute the plan
with the exhaustive flag for $200$ thousand letters faster than
the plan with the patient flag for $8$ million letters ($3$ minutes to $90$ minutes).
The algorithm runs in $5.39$ seconds while using the precomputed plan with exhaustive flag
on $200$ thousand letters.

\subsubsection*{Algorithm utilisation}

Our naive algorithm computes the calculation $P[i] \times T[j] \times (T[j]-P[i])^2$,
for every position of every alignment between $T$ and $P$.
For a text of $8$ millions numbers and a pattern of $100$ thousand numbers,
the naive algorithm runs in more than $10$ minutes.

Our tests showed that the Wildcard algorithm is always faster than the naive algorithm, even in small texts.





\subsection{Exact Hamming distance}


For a $8$ millions letters text and a $20$ thousands letters pattern, 
the algorithm runs in $4.1$ seconds on DNA sequences and in $5.7$ seconds on an English text.
% Can it be faster ?

We want to test:
\begin{itemize}
\setlength\itemsep{-0.4em}
\item How does the execution time vary with the pattern's form?
\item How good is the threshold sorting the letters as frequent or infrequent?
\item What is the best threshold on real text?
\end{itemize}



\subsubsection*{Pattern variation and execution time}

The algorithm is composed of two sections.
In the first section, the computation time depends
on the number of frequent letters in the pattern.
In the second section, the computation time depends on the number
of infrequent letters in the pattern and how often they appear in the text.

We want to know how the execution time can vary with the pattern,
so %for several sizes of pattern,
we made artificial patterns
with a minimum or maximum number of infrequent letter,
and inducing the computation of a defined number of FFT.
Let $L$ be the threshold with $L = \sqrt{m \log n}$.
In a pattern with the minimum number of infrequent letters ($0$),
all letters will appear more than $L$ times
(exactly $m/\sigma$ times).
In a pattern with the maximum number of infrequent letters,
we want to maximize the occurrences $k'$ of each infrequent letter
while still having the number of frequent letter we want.
We want the greater $k'<L$ such that $k > L$,
where $k$ in the number of occurrence of each frequent letter.
% We have:
% every frequent letter appear $k= max(L, (m - (L-1)* \sigma-X)/X)$
% every infrequent letter appear $(m-k*X)/(\sigma -X)$,
% such that $X$ is the number of frequent letters and $\sigma$ is the number of letters in the alphabet.

We ran the algorithm on patterns of increasing size, up to $20 000$ letters, inducing $1$ to $4$ FFTs,
with a minimum and maximum number of frequent letters, with $\sigma = 4$ (like a DNA pattern)
and $\sigma = 95$ (like an English pattern).
We also ran the algorithm when every letters of the pattern occur equally and $L=m$ such that no FFT is used.

The figure \ref{ExHDADN} presents the computation times on a $8$ millions letters DNA text
and the figure \ref{ExHDEnglish} the computation times on a $8$ millions English text.
The following times include the computation time without the initialisation nor the writing time in the output.
In this figures, a colored area indicates the expected execution time for a defined number of frequent letter.

\begin{figure}[h]
\includegraphics[scale=0.45]{./figures/ExHDADN.png}
\caption{DNA text of $8$ millions letters and DNA pattern of $0$ to $20 000$ letters.
Execution time for $0$ to $4$ frequent letters.}
\label{ExHDADN}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.45]{./figures/ExHDEnglish.png}
\caption{English text of $8$ millions letters and English pattern of $0$ to $20 000$ letters.
Execution time for $0$ to $4$ frequent letters.}
\label{ExHDEnglish}
\end{figure}


The algorithm always benefits of the use of at least one FFT when the pattern is longer than $1500$ letters.
As there are only $4$ different letters in a DNA pattern,
the computation time doesn't change when the pattern length increase and $4$ letters are frequents.

In a pattern matching on real DNA sequence, the $4$ letters appear with almost the same amount of occurrencies.
In those cases,
the algorithm will use $4$ FFT and will run in $2$ seconds
for a $8$ millions letters text and every size of pattern.

In a English text and pattern, it is more interesting to not use any FFT for a pattern smaller than $6000$ letters.

The curve for the utilisation of $0$ FFT (in green) increases more slowly in English text
than in DNA text.
This is due to the number of occurrences of each infrequent letters.
Assume that every infrequent letter appear with the same amount of occurrences.
In a English pattern, each $95$ letters appear $k'=m/95$ times,
while the $4$ DNA letters appear $k'=m/4$ times.
In the infrequent section, the algorithm looks for every pattern occurrences of the letters of each position,
thus $k'*m$ calculations:
we have $m^2/95$ calculations in the English text,
and $m^2/4$ calculations in the DNA text. 

The same idea is used to explain the difference between the maximal curves in the English and DNA texts.

% In a pattern matching on real English text, a few FFT are usually made, for example the characters space, carriage return, 'e'\ldots
% \textcolor{red}{what do I want to say with that ?}

The tests indicate a large variability of computation times.
If we don't know the letters' repartition as frequent or infrequent,
the hamming distance between a English text of $8$ millions letters
and a pattern of $20 000$ letters can be computed between $0.5$ s and $2.8$ seconds
(for up to $4$ FFT).
The threshold may be badly chosen and could be updated by considering the real FFT computation time.




\subsubsection*{Which threshold should we use while using real DNA or English texts?}

To find the best threshold to use,
we selected an English text of $8$ million letters
and ran the algorithm several times
with a $20 000$ letters pattern
randomly extracted from the text at each run.
We increased the threshold from $400$ to $2500$ occurrences.% with a 100 letters steps.

The results are gathered in the figure \ref{ExHDThresh} where
every point represents a run and its colour indicates the number of FFT used.
The blue line is the average computation time of $10$ runs.
It is used to find the best threshold:
the one inducing the smaller computation time.


\begin{figure}[h]
\includegraphics[scale=0.45]{./figures/ExHDThreshold.png}
\caption{Algorithm execution time for a $8$ millions letters English text
and English pattern of $0$ to $20 000$ letters.
Every point represents a run and its colour indicates the number of FFT used.
The blue curve is the average computation time of $10$ runs.
The red dotted line is the $\sqrt{m \log n}$ paper's threshold.}
\label{ExHDThresh}
\end{figure}


For a pattern of $20 000$ letters, the minimal computing time is on average $4,48$ seconds,
with a $1300$ letters threshold,
while the theoretical $\sqrt{m \log n}$ threshold was $678$ letters.
This new threshold pushes the algorithm to use $2$ or $3$ FFT.

All our tests with real patterns of different sizes produce this kind of curve:
a fast descent (up to $1100$ in the figure \ref{ExHDThresh})
then a flat section (from $1100$ to $1700$) and a small increase (from $1700$).
The flat section is represented by all the thresholds inducing an average computing time
which doesn't exceed the minimum computing time plus $0.03$ seconds.
The optimal threshold is in the flat section.
As a lucky random pattern can improve the average computation time
(as the fastest one with the $1300$ letters threshold),
we choose not to fix the threshold now, but give a range of the best threshold:
the flat section of the curve.

We ran the previous program on the $8$ millions letters of English text and patterns of increasing size
to estimate the optimal threshold depending on the pattern size.
We computed the average computation time of $50$ runs for each pattern size and threshold.

The figure \ref{ExHDThreshAll} shows the results on patterns of inscreasing size.
The light blue curve was made by taking the flat section's left end, 
the dark blue curve is the flat section's right end.
% Each curve represents a flat section's end of the threshold curve:
% the left end is the minimum curve, the right end is the maximum curve. 
For example in the $20 000$ letters pattern (figure \ref{ExHDThresh}),
the left end is the $1100$ letters threshold,
the right one is the $1700$ letters threshold.
We indicated the number of FFT used when the threshold is between the two thresholds.



\begin{figure}[h]
\includegraphics[scale=0.56]{./figures/ExHDThresholdAll.png}
\caption{Exact Hamming distance algorithm
running on a $8$ millions English text.
For a given pattern size,
a good threshold to use is between the two blue curves.
The number of FFT used is indicated in green.
The dotted red line indicates the $\sqrt{m \log n}$ paper's threshold.}
\label{ExHDThreshAll}
\end{figure}


This test shows that the threshold should be higher for every size of pattern.
\textcolor{red}{other thing to say ??}

We found this kind of figure for every size of English text.

For an English text of $8$ million letters and a $20 000$ English letters,
the algorithm runs now in $4.5$ seconds, 
when it ran in $5.7$ seconds with the $\sqrt{m \log n}$ paper's threshold.

\subsubsection*{Algorithm utilisation: comparion with the naive algorithm}

\textcolor{red}{Enought??}

We recommend to use the exact Hamming distance algorithm rather than the naive algorithm
when $\sigma$ is small or when we have long patterns. \textcolor{red}{how much ??}


\subsection{approximate Hamming distance}

\subsubsection*{Implementation}



We used the following random function to map the letters:
Select randomly a primary number $p$ greater than the alphabet size $\sigma$.
Then select randomly two numbers $a$ and $b$ smaller than $p$.
Every integer $i$ representing a letter is mapped to the number
$a * i + b \equiv \pmod p \equiv \pmod \sigma$.
We used the \textit{random} function of the \textit{stdlib} library
and generate a new seed for every mapping.
We saved the $1200$ first prime numbers in a file and load them at the beginning of the program.


We use our optimised version of the exact Hamming distance algorithm
running in $O(n \sqrt{m \log n})$ times
rather than the algorithm running in $O(\sigma \times n \log n)$ times mentionned in the paper.
The figure \ref{AppHDComp} shows how much time we save by using this algorithm
when $\epsilon$ changes.


% We implemented two Hamming distances in the approximate algorithm.
% The first one doing $\sigma$ FFT and running in $O(\sigma n \log n)$ times,
% the second one presented here, using an optimal number of FFT,
% depending of the letters frequency and running in $O(n \sqrt{m \log n})$ times.
% We already know that the second algorithm is faster
% and the figure \ref{AppHDComp} shows the computation time of the approximate algorithm
% when we use each Hamming distance algorithms.


\begin{figure}[h]
\includegraphics[scale=0.45]{./figures/appHDCompareNaive.png}
\caption{Comparison of the execution time between the optimised $O(\sigma n \log n)$ exact algorithm
(Fast Hamming distance)
and the $O(n \sqrt{m \log n})$ algorithm (Original Hamming distance).
The algorithms ran on a $8$ millions letters English text and a $5000$ letters pattern.
}
\label{AppHDComp}
\end{figure}

In the figure, when we have $\epsilon > 0.3$,
the optimum number of FFT is equal to the number of letter in the mapped text,
inducing equal running times.

% We will use the $O(n \sqrt{m \log n})$ optimised version of the exact Hamming distance
% algorithm in the later tests.
% rather than the $O(|\Sigma| n \log n)$ one presented in the paper.


\subsubsection*{Execution time}

This algorithm was tested on texts of $8$ millions letters,
a $10 000$ letters pattern and $\epsilon = 0.2$.

On a DNA text, the algorithms runs in around $30$ seconds.
On a English text, it runs in around $45$ seconds.
Those computations times are $10$ time slower than the exact Hamming distance algorithm,
due to the $22$ loop iterations.

% We compared the approximate's results to the exact's results
% and found that this algorithm approximates the Hamming distance by $\epsilon = 0.13$.


\subsubsection*{What does $2/\epsilon$ means?}

The table \ref{TableEpsilon} shows the number of letters in a text
when we use a $h: \Sigma \rightarrow \{1, 2, \ldots, 2/\epsilon\}$ mapping function.
It helps us calculate the number of FFT used in the approximate algorithm.


\begin{table}[h]
\centering
\begin{tabular}{|c|c||c|c||c|c|}
\hline
$\epsilon$ & $2/ \epsilon$ & $\epsilon$ & $2/ \epsilon$ & $\epsilon$ & $2/ \epsilon$ \\ \hline
$0.7$ & $3$ & $0.2$ & $10$ & $0.06$ & $33$ \\ \hline
$0.6$ & $3$ & $0.1$ & $20$ & $0.05$ & $40$ \\ \hline
$0.5$ & $4$ & $0.09$ & $22$ & $0.04$ & $50$ \\ \hline
$0.4$ & $5$ & $0.08$ & $25$ & $0.04$ & $67$ \\ \hline
$0.4$ & $7$ & $0.07$ & $29$ & $0.03$ & $100$ \\ \hline
\end{tabular}
\caption{Values of the rounded $2 / \epsilon$.
It represents the number of different letters after the mapping.}
\label{TableEpsilon}
\end{table}


% \subsubsection*{A simple optimization}





\subsubsection*{Can we do better than $\log n$ iterations?}
\label{AppIter}

For a $8$ millions letters English text, a $10 000$ letters pattern and $\epsilon = 0.2$,
the approximate algorithm runs in  $45$ seconds,
while using $\log n = 22$ loop iterations.
The computation time can be highly reduced by decreasing the number of iterations.
Note that the output error rate must stay under the input error rate $\epsilon$.
% \textcolor{red}{a reformuler}

We checked the approximate algorithm results
while we decrease the number of loop iterations. % in a approximate algorithm computation.
The runs with $22$ loop iterations and $\epsilon = 0.2$
produces an output error rate up to $0.087$
(we took the maximum error rate of more than $100$ runs),
we can reduce the number of iterations to come closer to the input error rate.

Our tests shows that $5$ iterations are enought
for those text and pattern sizes: the output error rate stays under $0.2$,
with a maximum of $0.107$, %(for more than $500$ runs),
while the running time decreases from $45$ to $16.8$ seconds.

 

\subsubsection*{How does the error rate affect the algorithm computation time?}

The algorithm ran while the pattern size and the error rate vary
and we compare its execution time to the naive algorithm's.
% The pattern is a part of the text selected randomly.
The algorithm uses $5$ iterations.
The figure \ref{AppHDTempsErr} shows the computation time of the algorithm.
Each point is the average time of $20$ runs.

\begin{figure}[h]
\includegraphics[scale=0.45]{./figures/appHDTempsError.png}
\caption{Approximate Hamming distance algorithm running on a $8$ millions letters of English text.
The English pattern and $\epsilon$ vary.
Comparison with the naive algorithm (blue line).}
\label{AppHDTempsErr}
\end{figure}

As expected, the smaller the value of $\epsilon$, the slower the execution time.
The naive algorithm is faster for pattern lenght under $2000$ letters.
The curves for $\epsilon = 0.5 / 0.4 / 0.3$ are almost flat
because the mapped text is composed of $4 / 5 / 6$ letters
and the Hamming distance algorithm uses the same number of FFT.
The approximate algorithm is faster than the naive algorithm
for patterns of length up to $3500$ letters with $\epsilon = 0.2$
and patterns of length up to $5500$ letters with $\epsilon = 0.1$.



\subsubsection*{Algorithm utilisation: Comparison with the two exact Hamming distance algorithms}

% When should we use the approximate algorithm instead of the exact Hamming distance?

Let see the number of FFT computed in each algorithm:
the $O(\sigma \times \sqrt{m \log n})$ exact Hamming distance uses $\sigma$ FFTs,
while the approximate algorithm uses $c \log n \times 2/\epsilon$ FFTs.
To use the approximate algorithm, we need to have $c \log n \times 2/\epsilon > \sigma$. 

For our English text, we have $c \times 2/\epsilon \times 22 > 95$.
We can use the original number of loop iterations with $c = 1$,
and be limited to $\epsilon > 0.46$,
or reduce it (without consequence on the result, see section \ref{AppIter})
with $c = 0.25$ and we can use the approximate algorithm for an error rate $\epsilon > 0.12$.

The $\epsilon$ limit changes if we have a bigger alphabet or a smaller text length.

On the other hand and looking at our tests,
it is always preferable to use the $O(n \sqrt{m \log n})$ exact Hamming distance algorithm
instead of the approximate algorithm as the exact Hamming distance uses the optimum number the FFTs at all times.







\subsection{k-mismatching $1$}

\subsubsection*{Implementation}

We used the data structures (SA, LCP) implementations from the \textit{geeksforgeeks} web site \cite{geek}.
The SA implementation is the fastest one to our knowledge. %not the fastest one as there are constant research to improve it.
The LCP structure is constructed in $O(n \log n)$,
slower than the $O(n)$ construction time mentionned in the paper,
but to our knowledge no $O(1)$ implementation exists.

We separate the SA construction time and the Kangaroo method execution time in the following tests.

\subsubsection*{Does the pattern length modify the computation time?}

On  a text of $8$ millions letters, a $1 000$ letters pattern and $5$ errors allowed,
the algorithm runs in $20$ seconds on English and DNA texts while the naive algorithm runs in $1$ second.
With patterns of $1$ million letters, the algorithm runs in $21.3$ seconds (average of $20$ runs).
The computation time doesn't increase much if we increase the length of the pattern.



\subsubsection*{Comparison with the naive algorithm and the exact Hamming distance algorithm}

We increased the value of $k$ 
while the algorithm ran on a $8$ millions letters DNA text and $1 000$ letters DNA pattern.
We calculated the k-mismatching algorithm's query time:
the time the algorithm uses to compute the solution when all its needed structures has been built.

The figure \ref{Kang} shows a comparison of computation times of
the k-mismatching algorithm, the naive algorithm and the optimized exact Hamming distance algorithm.
Each point represents the average time of $20$ runs.

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{./figures/Kang.png}
\caption{
Execution time of the k-mismatching algorithm $1$, exact Hamming distance algorithm and naive algorithm
when $k$ vary.
DNA text of $8$ millions letters and DNA patterns of a thousand letters.
}
\label{Kang}
\end{figure}


The full k-mismatching algorithm is always slower than the naive algorithm due to the construction time.

The computation time of the k-mismatching algorithm is better or similar to the naive one for $0$ and $1$ error.
For more than that, the naive algorithm is always faster.

The naive algorithm and the Hamming distance algorithm have similar execution time.
For DNA texts of $8$ millions letters, the naive algorithm is faster when $k < 260$,
for an English text, the naive algorithm is faster for $k < 180$ (data not shown). 

% Most of the naive algorithm and Hamming distance algorithm computation times
% are dedicated to write the result is a file ($0.83$ second).



\subsubsection*{Is there a text on which the k-mismatching algorithm is faster?}

We tried to find a type of text where the k-mismatching algorithm could be more efficient than the naive algorithm.
To do so, the naive algorithm should do as much comparisons as possible,
so the text and pattern should be close.
We made artificial texts where the entropy was very low:
for example a text composed of the letters $a$ and $b$ where the $b$ are found $0.1$\% of the time.
% of the time (we call it "ratio").

In a text of 8 millions letters, a pattern of 1 000 letters, k=5 and the $b/a$ ratio is 0.01. % (for 20 runs):
The k-mismatching algorithm runs in average in $15.7$ second, where the Kangaroo method runs in $4.6$ seconds.
the naive algorithm runs in average in $2.7$ seconds (between $1.5$ and $4.9$ seconds)
The naive algorithm is faster even in texts with a low entropy.



\subsubsection*{Algorithm utilisation}

Note that we didn't used the fastest structure build.
So if we consider only the query time of the k-mismatching algorithm $1$,
ours tests revealed that the naive algorithm and the exact Hamming distance algorithm
are faster than the Kangaroo method in English and DNA texts for $k > 1$,
and are always faster than the k-mismatching algorithm $1$. 






\subsection{K-mismatching $2$}

% \subsubsection*{Implementation}

% We used the data structures (SA, LCP) implementations from the \textit{geeksforgeeks} \cite{geek} web site.
% The SA implementation is the fastest one to our knowledge. %not the fastest one as there are constant research to improve it.
% The LCP structure is constructed in $O(n \log n)$,
% slower than the $O(n)$ construction time mentionned in the paper,
% but to our knowledge no $O(1)$ implementation exists.


\subsubsection*{Execution time}

We executed the algorithm on $8$ million letters text and $10 000$ letters patterns.
% We used a saved patient FFT plan.

On DNA text the algorithm runs in $3.1$ seconds.
As long as the $4$ letters are frequents, the running time stays constant.

On English text and $100$ errors allowed, the algorithm selects the Kangaroo method and runs in $14.14$ second.
For $1000$ errors allowed the algorithm selects the FFT method and runs in $20.24$ seconds
(with $41$ frequent letters).



\subsubsection*{Algorithm utilisation}
The running time of the FFT methods depends on the number of FFT it has to compute.
The running time of the Kangaroo method mostly depends on the structure construction time.

For a $8$ millions letters text and a $10 000$ letters pattern,
the FFT method needs around $0.5$ second per letters
while the Kangaroo methods needs $12.5$ seconds of construction time
and up to $2$ seconds of query time.
So for those length of files, we should use the Kangaroo methods
when the pattern is composed of more than $29$ frequent letters.

For DNA texts and other small alphabets, the FFT method should always be used.





\subsection{k-mismatching $3$}

\subsubsection*{Implementation}
We swapped the Approximate Hamming Distance algorithm 
to the exact Hamming Distance algorithm
to compute the pattern period,


% \textcolor{red}{WHAAT ?}
% We used the Approximate string matching algorithms instead of Karloff's algorithm.


\subsubsection*{Execution time}

The algorithm ran on English and DNA sequences of $100$ thousands letters with a $10 000$ letters pattern.
% We used our optimisations.

On the DNA text, and $k = 10$, the algorithm uses the Kangaroo method and runs in $7.8$ seconds.
With $k = 300$, the algorithm uses the RLE method and runs in less than $0.1$ second.

On the English text and $k = 10$, the algorithm uses the Kangaroo method and runs in $6.6$ seconds
With $k = 50$, the algorithm uses the RLE method and runs in $4.6$ second.

The main difference between the execution time of the runs using the RLE method %(runs $2$ and $4$)
comes from the number of infrequent letters in the text, thus the number of second derivative's updates.
In the DNA text, the $4$ letters are frequents so $4$ FFT are used,
with no additional second derivative updates.
In the English text, $6$ to $7$ letters are frequents, and we have a least $900$ thousands second derivative updates.



\subsubsection*{Threshold between the RLE and FFT methods}

The $8k$-period section of the algorithm sorts the patterns letters as frequents or infrequents
based on the number of run a character is present in. 
The algorithms uses the FFT method on frequent letters
and the RLE method on infrequent letters.
We want to find an estimation of this threshold $t$, undefined in the paper.

% The algorithm uses a threshold $t$ to sort the letters as frequent in infrequent,


% First, we tested the $8k$-period part to find the best threshold
% used to sort the characters as frequent or infrequent.
% $t$ is a threshold on the number of run a character is present in.

The RLE method's execution time depends on the number of pair of text/pattern run.
The FFT method's execution time depends on the number of frequent letters.

We ran the RLE section while increasing the number of pair of runs.
We compare it to the FFT section execution time for different FFT sizes.
The execution time of the FFT section includes the production of $T^*$ and $P^*$ strings
and the FFT method for one letter.
Tests show that if a second letter is computed, the execution time is close to twice the time for one letter
(results not shown).
% We used pre-computed patient plans. 
See figure \ref{OptiComp}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{./figures/OptiComparOptiNaivHD.png}
\caption{Execution time of the RLE method (yellow) while we increase the number of pair of text/pattern runs.
Times are compared to the execution time of the FFT method for $1$ FFT of $100$ thousands,
$1$ million and $2$ millions letters.}
\label{OptiComp}
\end{figure}

The FFT methods execution time stays constant while the number of pair of runs increase. 
The execution time of the RLE method grows linearly with the number of pairs of runs.
This curve and the FFT for $2$ millions letters cross at $55$ millions of pair of RLE runs.   

For an execution of the k-mismatching for a $1$ million letters pattern,
the threshold between frequent and infrequent letters is $55$ millions pairs of RLE runs
(we use a $2$ millions letters FFT for $1$ millions letters text).



% \subsubsection*{Comparison: K-mismatching $3$ and exact Hamming Distance algorithms while we increase the number of pair of runs}

% We want to compare the execution time of the k-mismatching $3$ algorithm to the exact Hamming distance algorithm.
% \textcolor{red}{why???}
% We created texts of $100$ differents letters and control the number of pair of runs
% between the text and pattern.

% The algorithms ran on a $1$ million letters text, a $100$ thousand letters pattern and $k = 100$.
% In the patterns, all letters appeared with the same frequency,
% but we forced the utilisation of $0$ and $1$ FFT in both algorithm.
% The execution time of the exact Hamming distance and the k-mismatching are represented in the figure \ref{Opti01FFT}.


% \begin{figure}[h]
% \centering
% \includegraphics[scale=0.45]{./figures/Opti01FFT.png}
% \caption{Comparison of k-mismatching $3$ and the exact Hamming distance execution times.
% Text of $1$ million letters and pattern of $100$ thousand letters with $100$ differents letters.
% We forced the utilisation of $0$ and $1$ FFT.}
% \label{Opti01FFT}
% \end{figure}

% For the runs with $0$ or $1$ FFT,
% the execution time of the algorithms are similar around $300$ millions pairs of runs:
% the k-mismatching is faster under $300$ millions pair of runs.
% The same kind of curves were found for up to $20$ FFT, they also intersect around $300$ millions of runs
% (results not shown).

% For a $1$ millions letters text, a $100$ thousands letters pattern
% and $k$ big enough so the k-mismatching algorithm uses the FFT+RLE method,
% the $k$-mismatching algorithm runs faster when we have less than $300$ millions of pair of runs,
% for any frequent letters.



\subsubsection*{Comparison of the k-mismatching $3$ and exact Hamming Distance algorithms on real files}

We ran the $k$-mismatching $3$ algorithm
on $1$ million letters texts and $100$ thousands letters patterns.
We compare its execution time to the exact Hamming distance algorithm execution time.

On DNA files, the exact Hamming distance algorithm runs in $0.44$ second.
The k-mismatching use the Kangaroo method until $k \approx 6500$,
the LCP construction alone needs several seconds.
When $k>6500$, the RLE method is used, the algorithm runs in $1.67$ second.
As all letters are frequent, both algorithms use $4$ FFT.
The additional computation time of the k-mismatching comes from additionnal calculation: the period pattern calculation,
the computation of $T^*$, $P^*$\ldots

On English text, the exact Hamming distance algorithm runs in $1.5$ second.
The k-mismatching use the Kangaroo method until $k \approx 20000$,
the LCP construction alone needs several dozens of seconds.
When $k>20000$, the RLE method is used, the algorithm runs in $7.5$ second.
$18$ letters are frequent, but text and pattern aren't periodical enough to profit from the RLE method.



\subsubsection*{Algorithm utilisation}

The exact Hamming Distance algorithm is always faster that the k-mismatching algorithm for short alphabet, as DNA.

On real DNA and English files,
we recommend to use the exact Hamming distance algorithm rather than the k-mismatching $3$ algorithm.

Text and pattern should have a good periodicity, otherwise, the Kangaroo method is too slow.
Note that we didn't used the fastest LCP construction method.








\section*{Conclusion}

We implemented and compared $6$ algorithms solving several pattern matching problems: 
exact matching, exact matching with wildcards, approximate matching and k-mismatching.
% Using the FFTW3 library 
% use saved plan
We changed all thresholds to optimise the algorithms execution time.

For the exact matching with wildcards, Clifford and Clifford's \cite{WC} algorithm is faster than the naive algorithm.
But for every other problems, Abrahamson's algorithm \cite{ExactHD} is a good algorithm to use
in most situations.


% HD algorithm is good to solve the exact matching problem for small alphabet or long pattern.


% the exact algorithm is better than the approximate algorithm  





All algorithms, their implementation and results can be found on our website \cite{stringipedia}.


We hope to include the k-mismatching algorithm from Clifford et al. \cite{Undone} in future work.
This algorithm uses the RLE method.
We also would like to implement the LCP structure in $O(1)$,
and update our results.


\section*{Bibliography}

\bibliographystyle{abbrv}
\bibliography{biblio}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

% \begin{biblio}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.